{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe1cd34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbe1cd34",
        "outputId": "8e73a2ec-5233-4a25-ff0f-5cb4e2d69af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qvAj0SdwH0Cw",
      "metadata": {
        "id": "qvAj0SdwH0Cw"
      },
      "outputs": [],
      "source": [
        "\n",
        "#LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ec9b2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4ec9b2f",
        "outputId": "3dbf1f22-b259-4819-e2dc-c6a71aa3807f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Pressure classification: 0.98\n",
            "Accuracy for Zone classification: 0.98\n",
            "Accuracy for Top Margin classification: 1.00\n",
            "Accuracy for Letter Size classification: 1.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/majorProject/dataset.csv')  # Replace 'your_dataset.csv' with the actual file path\n",
        "data1= pd.read_csv('/content/drive/MyDrive/majorProject/datasetb.csv')\n",
        "X_margin = data1[['margin']]\n",
        "y_margin = data1['Top_Margin']\n",
        "\n",
        "X_size = data1[['size']]\n",
        "y_size = data1['Letter_Size']\n",
        "# Extract features (Average, Percentage) and target labels (Pressure, Zone)\n",
        "\n",
        "X_pre = data[['Average']]\n",
        "X_zone = data[['Zone Above', 'Zone Middle', 'Zone Below']]\n",
        "\n",
        "y_pressure = data['Pressure']\n",
        "y_zone = data['Zone']\n",
        "\n",
        "# Split the data into training and testing sets (e.g., 80% for training, 20% for testing)\n",
        "X_train_pre, X_test_pre, y_train_pressure, y_test_pressure = train_test_split(X_pre, y_pressure,test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_zone, X_test_zone,y_train_zone, y_test_zone = train_test_split(X_zone, y_zone, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_margin, X_test_margin,y_train_margin, y_test_margin = train_test_split(X_margin, y_margin, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_size, X_test_size,y_train_size, y_test_size = train_test_split(X_size, y_size, test_size=0.2, random_state=42)\n",
        "# Train a logistic regression model for Pressure classification\n",
        "model_pressure = LogisticRegression()\n",
        "model_pressure.fit(X_train_pre, y_train_pressure)\n",
        "\n",
        "# Train a logistic regression model for Zone classification\n",
        "model_zone = LogisticRegression()\n",
        "model_zone.fit(X_train_zone, y_train_zone)\n",
        "\n",
        "model_margin = LogisticRegression()\n",
        "model_margin.fit(X_train_margin,y_train_margin)\n",
        "\n",
        "model_size = LogisticRegression()\n",
        "model_size.fit(X_train_size,y_train_size)\n",
        "\n",
        "# Predict on the test set for both Pressure and Zone\n",
        "y_pred_pressure = model_pressure.predict(X_test_pre)\n",
        "y_pred_zone = model_zone.predict(X_test_zone)\n",
        "y_pred_margin = model_margin.predict(X_test_margin)\n",
        "y_pred_size = model_size.predict(X_test_size)\n",
        "# Calculate and print the accuracy for Pressure and Zone classification\n",
        "accuracy_pressure = accuracy_score(y_test_pressure, y_pred_pressure)\n",
        "accuracy_zone = accuracy_score(y_test_zone, y_pred_zone)\n",
        "accuracy_margin = accuracy_score(y_test_margin, y_pred_margin)\n",
        "accuracy_size = accuracy_score(y_test_size, y_pred_size)\n",
        "print(f'Accuracy for Pressure classification: {accuracy_pressure:.2f}')\n",
        "print(f'Accuracy for Zone classification: {accuracy_zone:.2f}')\n",
        "print(f'Accuracy for Top Margin classification: {accuracy_margin:.2f}')\n",
        "print(f'Accuracy for Letter Size classification: {accuracy_size:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05720094",
      "metadata": {
        "id": "05720094"
      },
      "outputs": [],
      "source": [
        "#NAIVE BAYES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3abf9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e3abf9b",
        "outputId": "b07e678d-59dd-4ebf-f409-08091e9ec162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Pressure classification: 0.98\n",
            "Accuracy for Zone classification: 0.73\n",
            "Accuracy for Top Margin classification: 1.00\n",
            "Accuracy for Letter Size classification: 0.97\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/majorProject/dataset.csv')  # Replace 'your_dataset.csv' with the actual file path\n",
        "data1= pd.read_csv('/content/drive/MyDrive/majorProject/datasetb.csv')\n",
        "\n",
        "X_margin = data1[['margin']]\n",
        "y_margin = data1['Top_Margin']\n",
        "\n",
        "X_size = data1[['size']]\n",
        "y_size = data1['Letter_Size']\n",
        "\n",
        "# Extract features (Average, Percentage) and target labels (Pressure, Zone)\n",
        "X_pre = data[['Average']]\n",
        "X_zone = data[['Zone Above', 'Zone Middle', 'Zone Below']]\n",
        "\n",
        "y_pressure = data['Pressure']\n",
        "y_zone = data['Zone']\n",
        "\n",
        "# Split the data into training and testing sets (e.g., 80% for training, 20% for testing)\n",
        "X_train_pre, X_test_pre, y_train_pressure, y_test_pressure = train_test_split(X_pre, y_pressure, test_size=0.2, random_state=42)\n",
        "X_train_zone, X_test_zone, y_train_zone, y_test_zone = train_test_split(X_zone, y_zone, test_size=0.2, random_state=42)\n",
        "X_train_margin, X_test_margin, y_train_margin, y_test_margin = train_test_split(X_margin, y_margin, test_size=0.2, random_state=42)\n",
        "X_train_size, X_test_size, y_train_size, y_test_size = train_test_split(X_size, y_size, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naive Bayes model for Pressure classification\n",
        "model_pressure = GaussianNB()\n",
        "model_pressure.fit(X_train_pre, y_train_pressure)\n",
        "\n",
        "# Train a Gaussian Naive Bayes model for Zone classification\n",
        "model_zone = GaussianNB()\n",
        "model_zone.fit(X_train_zone, y_train_zone)\n",
        "\n",
        "# Train a Gaussian Naive Bayes model for Margin classification\n",
        "model_margin = GaussianNB()\n",
        "model_margin.fit(X_train_margin, y_train_margin)\n",
        "\n",
        "# Train a Gaussian Naive Bayes model for Letter Size classification\n",
        "model_size = GaussianNB()\n",
        "model_size.fit(X_train_size, y_train_size)\n",
        "\n",
        "# Predict on the test set for each classification\n",
        "y_pred_pressure = model_pressure.predict(X_test_pre)\n",
        "y_pred_zone = model_zone.predict(X_test_zone)\n",
        "y_pred_margin = model_margin.predict(X_test_margin)\n",
        "y_pred_size = model_size.predict(X_test_size)\n",
        "\n",
        "# Calculate and print the accuracy for each classification\n",
        "accuracy_pressure = accuracy_score(y_test_pressure, y_pred_pressure)\n",
        "accuracy_zone = accuracy_score(y_test_zone, y_pred_zone)\n",
        "accuracy_margin = accuracy_score(y_test_margin, y_pred_margin)\n",
        "accuracy_size = accuracy_score(y_test_size, y_pred_size)\n",
        "\n",
        "print(f'Accuracy for Pressure classification: {accuracy_pressure:.2f}')\n",
        "print(f'Accuracy for Zone classification: {accuracy_zone:.2f}')\n",
        "print(f'Accuracy for Top Margin classification: {accuracy_margin:.2f}')\n",
        "print(f'Accuracy for Letter Size classification: {accuracy_size:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e20d90",
      "metadata": {
        "id": "28e20d90"
      },
      "outputs": [],
      "source": [
        "#Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d7c1ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64d7c1ea",
        "outputId": "e25fa918-3500-41c7-b86c-dc43c95155f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy for Pressure classification: 0.98\n",
            "Average Accuracy for Zone classification: 0.92\n",
            "Average Accuracy for Top Margin classification: 1.00\n",
            "Average Accuracy for Letter Size classification: 1.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/majorProject/dataset.csv')\n",
        "data1 = pd.read_csv('/content/drive/MyDrive/majorProject/datasetb.csv')\n",
        "\n",
        "X_margin = data1[['margin']]\n",
        "y_margin = data1['Top_Margin']\n",
        "\n",
        "X_size = data1[['size']]\n",
        "y_size = data1['Letter_Size']\n",
        "\n",
        "# Extract features (Average, Percentage) and target labels (Pressure, Zone)\n",
        "X_pre = data[['Average']]\n",
        "X_zone = data[['Zone Above', 'Zone Middle', 'Zone Below']]\n",
        "\n",
        "y_pressure = data['Pressure']\n",
        "y_zone = data['Zone']\n",
        "\n",
        "# Define the number of folds for cross-validation\n",
        "num_folds = 5\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "accuracy_pressure_scores = []\n",
        "accuracy_zone_scores = []\n",
        "accuracy_margin_scores = []\n",
        "accuracy_size_scores = []\n",
        "\n",
        "for train_index, test_index in kfold.split(X_pre):\n",
        "    X_train_pre, X_test_pre = X_pre.iloc[train_index], X_pre.iloc[test_index]\n",
        "    y_train_pressure, y_test_pressure = y_pressure.iloc[train_index], y_pressure.iloc[test_index]\n",
        "\n",
        "    # Train a Random Forest model for Pressure classification\n",
        "    model_pressure = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model_pressure.fit(X_train_pre, y_train_pressure)\n",
        "    y_pred_pressure = model_pressure.predict(X_test_pre)\n",
        "    accuracy_pressure = accuracy_score(y_test_pressure, y_pred_pressure)\n",
        "    accuracy_pressure_scores.append(accuracy_pressure)\n",
        "\n",
        "for train_index, test_index in kfold.split(X_zone):\n",
        "    X_train_zone, X_test_zone = X_zone.iloc[train_index], X_zone.iloc[test_index]\n",
        "    y_train_zone, y_test_zone = y_zone.iloc[train_index], y_zone.iloc[test_index]\n",
        "    # Train a Random Forest model for Zone classification\n",
        "    model_zone = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model_zone.fit(X_train_zone, y_train_zone)\n",
        "    y_pred_zone = model_zone.predict(X_test_zone)\n",
        "    accuracy_zone = accuracy_score(y_test_zone, y_pred_zone)\n",
        "    accuracy_zone_scores.append(accuracy_zone)\n",
        "\n",
        "for train_index, test_index in kfold.split(X_margin):\n",
        "    X_train_margin, X_test_margin = X_margin.iloc[train_index], X_margin.iloc[test_index]\n",
        "    y_train_margin, y_test_margin = y_margin.iloc[train_index], y_margin.iloc[test_index]\n",
        "    # Train a Random Forest model for Margin classification\n",
        "    model_margin = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model_margin.fit(X_train_margin, y_train_margin)\n",
        "    y_pred_margin = model_margin.predict(X_test_margin)\n",
        "    accuracy_margin = accuracy_score(y_test_margin, y_pred_margin)\n",
        "    accuracy_margin_scores.append(accuracy_margin)\n",
        "\n",
        "for train_index, test_index in kfold.split(X_size):\n",
        "    X_train_size, X_test_size = X_size.iloc[train_index], X_size.iloc[test_index]\n",
        "    y_train_size, y_test_size = y_size.iloc[train_index], y_size.iloc[test_index]\n",
        "    # Train a Random Forest model for Letter Size classification\n",
        "    model_size = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model_size.fit(X_train_size, y_train_size)\n",
        "    y_pred_size = model_size.predict(X_test_size)\n",
        "    accuracy_size = accuracy_score(y_test_size, y_pred_size)\n",
        "    accuracy_size_scores.append(accuracy_size)\n",
        "\n",
        "# Calculate and print the average accuracy for Pressure, Zone, Margin, and Letter Size classification\n",
        "avg_accuracy_pressure = sum(accuracy_pressure_scores) / num_folds\n",
        "avg_accuracy_zone = sum(accuracy_zone_scores) / num_folds\n",
        "avg_accuracy_margin = sum(accuracy_margin_scores) / num_folds\n",
        "avg_accuracy_size = sum(accuracy_size_scores) / num_folds\n",
        "\n",
        "print(f'Average Accuracy for Pressure classification: {avg_accuracy_pressure:.2f}')\n",
        "print(f'Average Accuracy for Zone classification: {avg_accuracy_zone:.2f}')\n",
        "print(f'Average Accuracy for Top Margin classification: {avg_accuracy_margin:.2f}')\n",
        "print(f'Average Accuracy for Letter Size classification: {avg_accuracy_size:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e31b24",
      "metadata": {
        "id": "a6e31b24"
      },
      "outputs": [],
      "source": [
        "#Decicsion Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f3a640",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92f3a640",
        "outputId": "77ce1a02-3b5e-4572-efed-7f2dfb5e7566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Pressure classification: 0.98\n",
            "Accuracy for Zone classification: 0.90\n",
            "Accuracy for Top Margin classification: 1.00\n",
            "Accuracy for Letter Size classification: 1.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/majorProject/dataset.csv')  # Replace 'your_dataset.csv' with the actual file path\n",
        "data1 = pd.read_csv('/content/drive/MyDrive/majorProject/datasetb.csv')\n",
        "\n",
        "X_margin = data1[['margin']]\n",
        "y_margin = data1['Top_Margin']\n",
        "\n",
        "X_size = data1[['size']]\n",
        "y_size = data1['Letter_Size']\n",
        "\n",
        "# Extract features (Average, Percentage) and target labels (Pressure, Zone)\n",
        "X_pre = data[['Average']]\n",
        "X_zone = data[['Zone Above', 'Zone Middle', 'Zone Below']]\n",
        "\n",
        "y_pressure = data['Pressure']\n",
        "y_zone = data['Zone']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_pre, X_test_pre, y_train_pressure, y_test_pressure = train_test_split(\n",
        "    X_pre, y_pressure, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_zone, X_test_zone, y_train_zone, y_test_zone = train_test_split(\n",
        "    X_zone, y_zone, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_margin, X_test_margin, y_train_margin, y_test_margin = train_test_split(\n",
        "    X_margin, y_margin, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_size, X_test_size, y_train_size, y_test_size = train_test_split(\n",
        "    X_size, y_size, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree model for Pressure classification\n",
        "model_pressure = DecisionTreeClassifier(random_state=42)\n",
        "model_pressure.fit(X_train_pre, y_train_pressure)\n",
        "\n",
        "# Train a Decision Tree model for Zone classification\n",
        "model_zone = DecisionTreeClassifier(random_state=42)\n",
        "model_zone.fit(X_train_zone, y_train_zone)\n",
        "\n",
        "# Train a Decision Tree model for Margin classification\n",
        "model_margin = DecisionTreeClassifier(random_state=42)\n",
        "model_margin.fit(X_train_margin, y_train_margin)\n",
        "\n",
        "# Train a Decision Tree model for Letter Size classification\n",
        "model_size = DecisionTreeClassifier(random_state=42)\n",
        "model_size.fit(X_train_size, y_train_size)\n",
        "\n",
        "# Predict on the test set for each classification\n",
        "y_pred_pressure = model_pressure.predict(X_test_pre)\n",
        "y_pred_zone = model_zone.predict(X_test_zone)\n",
        "y_pred_margin = model_margin.predict(X_test_margin)\n",
        "y_pred_size = model_size.predict(X_test_size)\n",
        "\n",
        "# Calculate and print the accuracy for each classification\n",
        "accuracy_pressure = accuracy_score(y_test_pressure, y_pred_pressure)\n",
        "accuracy_zone = accuracy_score(y_test_zone, y_pred_zone)\n",
        "accuracy_margin = accuracy_score(y_test_margin, y_pred_margin)\n",
        "accuracy_size = accuracy_score(y_test_size, y_pred_size)\n",
        "\n",
        "print(f'Accuracy for Pressure classification: {accuracy_pressure:.2f}')\n",
        "print(f'Accuracy for Zone classification: {accuracy_zone:.2f}')\n",
        "print(f'Accuracy for Top Margin classification: {accuracy_margin:.2f}')\n",
        "print(f'Accuracy for Letter Size classification: {accuracy_size:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5272bed",
      "metadata": {
        "id": "f5272bed"
      },
      "outputs": [],
      "source": [
        "#SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6108e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a6108e6",
        "outputId": "fe252030-e799-4a1c-9e62-c20fb4ecdd34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for Pressure classification: 0.98\n",
            "Accuracy for Zone classification: 0.98\n",
            " Accuracy For Top Margin Classification: 0.99\n",
            " Accuracy For Letter Size Classification: 0.99\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def train_model_pre(data):\n",
        "    # Extracting features and labels\n",
        "    X = data[['Average']].values\n",
        "    y = data['Pressure'].map({'Heavy': 1, 'Medium': 2, 'Light': 3}).values\n",
        "\n",
        "    # Initializing SVC model\n",
        "    svc_model = SVC(C=100, gamma=0.0001, kernel='rbf')\n",
        "\n",
        "    # Cross-validation\n",
        "    accuracy_scores = cross_val_score(svc_model, X, y, cv=5)\n",
        "\n",
        "    # Calculating mean accuracy\n",
        "    mean_accuracy = np.mean(accuracy_scores)\n",
        "\n",
        "    # Fitting the model to the entire dataset\n",
        "    svc_model.fit(X, y)\n",
        "\n",
        "    # Save the model to a .pkl file\n",
        "    #joblib.dump(svc_model, save_model_path)\n",
        "\n",
        "    return mean_accuracy\n",
        "\n",
        "\n",
        "def train_model_margin(data):\n",
        "    # Extracting features and labels\n",
        "    X = data[['margin']].values\n",
        "    y = data['Top_Margin'].map({'NARROW': 1, 'BIG': 2}).values\n",
        "\n",
        "    # Initializing SVC model\n",
        "    svc_model = SVC(C=100, gamma=0.0001, kernel='rbf')\n",
        "\n",
        "    # Cross-validation\n",
        "    accuracy_scores = cross_val_score(svc_model, X, y, cv=5)\n",
        "\n",
        "    # Calculating mean accuracy\n",
        "    mean_accuracy = np.mean(accuracy_scores)\n",
        "\n",
        "    # Fitting the model to the entire dataset\n",
        "    svc_model.fit(X, y)\n",
        "\n",
        "    # Save the model to a .pkl file\n",
        "    #joblib.dump(svc_model, save_model_path)\n",
        "\n",
        "    return mean_accuracy\n",
        "\n",
        "def train_model_size(data):\n",
        "    # Extracting features and labels\n",
        "    X = data[['size']].values\n",
        "    y = data['Letter_Size'].map({'SMALL': 1, 'MEDIUM': 2, 'BIG': 3}).values\n",
        "\n",
        "    # Initializing SVC model\n",
        "    svc_model = SVC(C=100, gamma=0.0001, kernel='rbf')\n",
        "\n",
        "    # Cross-validation\n",
        "    accuracy_scores = cross_val_score(svc_model, X, y, cv=5)\n",
        "\n",
        "    # Calculating mean accuracy\n",
        "    mean_accuracy = np.mean(accuracy_scores)\n",
        "\n",
        "    # Fitting the model to the entire dataset\n",
        "    svc_model.fit(X, y)\n",
        "\n",
        "    # Save the model to a .pkl file\n",
        "    #joblib.dump(svc_model, save_model_path)\n",
        "\n",
        "    return mean_accuracy\n",
        "\n",
        "\n",
        "\n",
        "def train_model_zone(data):\n",
        "    # Extracting features and labels\n",
        "    X = data[['Zone Above', 'Zone Middle', 'Zone Below']].values\n",
        "    y = data['Zone'].map({'Above': 1, 'Middle': 2, 'Below': 3}).values\n",
        "\n",
        "    # Initializing SVC model\n",
        "    svc_model = SVC(C=100, gamma=0.0001, kernel='rbf')\n",
        "\n",
        "    # Cross-validation\n",
        "    accuracy_scores = cross_val_score(svc_model, X, y, cv=5)\n",
        "\n",
        "    # Calculating mean accuracy\n",
        "    mean_accuracy = np.mean(accuracy_scores)\n",
        "\n",
        "    # Fitting the model to the entire dataset\n",
        "    svc_model.fit(X, y)\n",
        "\n",
        "    # Save the model to a .pkl file\n",
        "    #joblib.dump(svc_model, save_model_path)\n",
        "\n",
        "    return mean_accuracy\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/majorProject/dataset.csv')  # Replace 'your_dataset.csv' with the actual file path\n",
        "data1= pd.read_csv('/content/drive/MyDrive/majorProject/datasetb.csv')\n",
        "\n",
        "\n",
        "accuracy_pressure_svm = train_model_pre(data)\n",
        "accuracy_zone_svm = train_model_zone(data)\n",
        "accuracy_margin_svm = train_model_margin(data1)\n",
        "accuracy_size_svm = train_model_size(data1)\n",
        "print(f'Accuracy for Pressure classification: {accuracy_pressure_svm:.2f}')\n",
        "print(f'Accuracy for Zone classification: {accuracy_zone_svm:.2f}')\n",
        "print(f\" Accuracy For Top Margin Classification: {accuracy_margin_svm:.2f}\")\n",
        "print(f\" Accuracy For Letter Size Classification: {accuracy_margin_svm:.2f}\")\n",
        "\n",
        "#print(\"Trained SVC Model saved at:\", model_save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mivVhpTvxrEA",
      "metadata": {
        "id": "mivVhpTvxrEA"
      },
      "outputs": [],
      "source": [
        "#Neural Network(Keras Sequential Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nNc-a7vrALMq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nNc-a7vrALMq",
        "outputId": "555e9c0e-b510-4b68-b9e0-8f8605ef8b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " PRESSURE MODEL COMPILATION \n",
            "Epoch 1/10\n",
            "37/37 [==============================] - 1s 10ms/step - loss: -55.7962 - accuracy: 0.3271 - val_loss: -105.9946 - val_accuracy: 0.3716\n",
            "Epoch 2/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -190.6762 - accuracy: 0.3271 - val_loss: -287.1176 - val_accuracy: 0.3716\n",
            "Epoch 3/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -471.4693 - accuracy: 0.3271 - val_loss: -671.2264 - val_accuracy: 0.3716\n",
            "Epoch 4/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -1025.7590 - accuracy: 0.3271 - val_loss: -1381.1698 - val_accuracy: 0.3716\n",
            "Epoch 5/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -1996.7089 - accuracy: 0.3271 - val_loss: -2565.4805 - val_accuracy: 0.3716\n",
            "Epoch 6/10\n",
            "37/37 [==============================] - 0s 4ms/step - loss: -3520.6497 - accuracy: 0.3271 - val_loss: -4350.8662 - val_accuracy: 0.3716\n",
            "Epoch 7/10\n",
            "37/37 [==============================] - 0s 4ms/step - loss: -5739.3145 - accuracy: 0.3271 - val_loss: -6864.9712 - val_accuracy: 0.3716\n",
            "Epoch 8/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -8796.5674 - accuracy: 0.3271 - val_loss: -10250.0947 - val_accuracy: 0.3716\n",
            "Epoch 9/10\n",
            "37/37 [==============================] - 0s 4ms/step - loss: -12844.8721 - accuracy: 0.3271 - val_loss: -14615.7266 - val_accuracy: 0.3716\n",
            "Epoch 10/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -17966.9160 - accuracy: 0.3271 - val_loss: -20128.3477 - val_accuracy: 0.3716\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            " ZONE MODEL COMPILATION \n",
            "Epoch 1/10\n",
            "37/37 [==============================] - 1s 8ms/step - loss: -39.5287 - accuracy: 0.3246 - val_loss: -89.9155 - val_accuracy: 0.3581\n",
            "Epoch 2/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -213.0592 - accuracy: 0.3254 - val_loss: -364.6642 - val_accuracy: 0.3581\n",
            "Epoch 3/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -723.4904 - accuracy: 0.3254 - val_loss: -1124.9369 - val_accuracy: 0.3581\n",
            "Epoch 4/10\n",
            "37/37 [==============================] - 0s 3ms/step - loss: -1949.7411 - accuracy: 0.3254 - val_loss: -2785.6721 - val_accuracy: 0.3581\n",
            "Epoch 5/10\n",
            "37/37 [==============================] - 0s 4ms/step - loss: -4385.0645 - accuracy: 0.3254 - val_loss: -5817.7329 - val_accuracy: 0.3581\n",
            "Epoch 6/10\n",
            "37/37 [==============================] - 0s 6ms/step - loss: -8527.2100 - accuracy: 0.3254 - val_loss: -10593.6201 - val_accuracy: 0.3581\n",
            "Epoch 7/10\n",
            "37/37 [==============================] - 0s 6ms/step - loss: -14742.5420 - accuracy: 0.3254 - val_loss: -17610.4473 - val_accuracy: 0.3581\n",
            "Epoch 8/10\n",
            "37/37 [==============================] - 0s 5ms/step - loss: -23484.6094 - accuracy: 0.3254 - val_loss: -27150.8984 - val_accuracy: 0.3581\n",
            "Epoch 9/10\n",
            "37/37 [==============================] - 0s 5ms/step - loss: -35130.6328 - accuracy: 0.3254 - val_loss: -39684.2930 - val_accuracy: 0.3581\n",
            "Epoch 10/10\n",
            "37/37 [==============================] - 0s 6ms/step - loss: -50235.5898 - accuracy: 0.3254 - val_loss: -55485.2695 - val_accuracy: 0.3581\n",
            "12/12 [==============================] - 0s 3ms/step\n",
            " MARGIN MODEL COMPILATION \n",
            "Epoch 1/10\n",
            "31/31 [==============================] - 2s 16ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "31/31 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "31/31 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "31/31 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "31/31 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "31/31 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            " SIZE MODEL COMPILATION \n",
            "Epoch 1/10\n",
            "31/31 [==============================] - 1s 9ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "31/31 [==============================] - 0s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "31/31 [==============================] - 0s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Accuracy for Pressure classification: 0.32\n",
            "Accuracy for Zone classification: 0.34\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [0, 307]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-81d174cb68a9>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Evaluate the model on the cleaned test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0maccuracy_margin_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_margin_cleaned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_margin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy for Margin classification after removing NaN values: {accuracy_margin_cleaned:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 307]"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/majorProject/dataset.csv')\n",
        "data1 = pd.read_csv('/content/drive/MyDrive/majorProject/datasetb.csv')\n",
        "\n",
        "# Extract features and labels\n",
        "X_pre = data[['Average']]\n",
        "X_zone = data[['Zone Above', 'Zone Middle', 'Zone Below']]\n",
        "X_margin = data1[['margin']]\n",
        "X_size = data1[['size']]\n",
        "\n",
        "# Convert labels to numerical values\n",
        "class_label_pressure = data['Pressure'].map({'Heavy': 1, 'Medium': 2, 'Light': 3}).values\n",
        "class_label_zone = data['Zone'].map({'Above': 1, 'Middle': 2, 'Below': 3}).values\n",
        "class_label_margin = data1['Top_Margin'].map({'DESCENDING': 1, 'ASCENDING': 2, 'STRAIGHT': 3}).values\n",
        "class_label_size = data1['Letter_Size'].map({'Small': 1, 'Medium': 2, 'Large': 3}).values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train_pre, X_test_pre, y_train_pressure, y_test_pressure = train_test_split(X_pre, class_label_pressure, test_size=0.2, random_state=42)\n",
        "X_train_zone, X_test_zone, y_train_zone, y_test_zone = train_test_split(X_zone, class_label_zone, test_size=0.2, random_state=42)\n",
        "X_train_margin, X_test_margin, y_train_margin, y_test_margin = train_test_split(X_margin, class_label_margin, test_size=0.2, random_state=42)\n",
        "X_train_size, X_test_size, y_train_size, y_test_size = train_test_split(X_size, class_label_size, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the neural network architecture for Pressure classification\n",
        "model_pressure = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_pre.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Define the neural network architecture for Zone classification\n",
        "model_zone = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_zone.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Define the neural network architecture for Margin classification\n",
        "model_margin = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_margin.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Define the neural network architecture for Size classification\n",
        "model_size = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_size.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the models\n",
        "model_pressure.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_zone.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_margin.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_size.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(''' PRESSURE MODEL COMPILATION ''')\n",
        "# Train the Pressure model\n",
        "model_pressure.fit(X_train_pre, y_train_pressure, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on the test set for Pressure classification\n",
        "y_pred_pressure = (model_pressure.predict(X_test_pre) > 0.5).astype(\"int32\")  # Thresholding for binary classification\n",
        "\n",
        "print(''' ZONE MODEL COMPILATION ''')\n",
        "# Train the Zone model\n",
        "model_zone.fit(X_train_zone, y_train_zone, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on the test set for Zone classification\n",
        "y_pred_zone = (model_zone.predict(X_test_zone) > 0.5).astype(\"int32\")  # Thresholding for binary classification\n",
        "\n",
        "print(''' MARGIN MODEL COMPILATION ''')\n",
        "# Train the Margin model\n",
        "model_margin.fit(X_train_margin, y_train_margin, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on the test set for Margin classification\n",
        "y_pred_margin = (model_margin.predict(X_test_margin) > 0.5).astype(\"int32\")  # Thresholding for binary classification\n",
        "\n",
        "print(''' SIZE MODEL COMPILATION ''')\n",
        "# Train the Size model\n",
        "model_size.fit(X_train_size, y_train_size, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on the test set for Size classification\n",
        "y_pred_size = (model_size.predict(X_test_size) > 0.5).astype(\"int32\")  # Thresholding for binary classification\n",
        "\n",
        "# Evaluate the models\n",
        "accuracy_pressure = accuracy_score(y_test_pressure, y_pred_pressure)\n",
        "print(f'Accuracy for Pressure classification: {accuracy_pressure:.2f}')\n",
        "\n",
        "accuracy_zone = accuracy_score(y_test_zone, y_pred_zone)\n",
        "print(f'Accuracy for Zone classification: {accuracy_zone:.2f}')\n",
        "\n",
        "# Identify rows with NaN values in y_test_margin\n",
        "nan_indices = np.isnan(y_test_margin)\n",
        "\n",
        "# Remove corresponding rows from y_test_margin and X_test_margin\n",
        "y_test_margin_cleaned = y_test_margin[~nan_indices]\n",
        "X_test_margin_cleaned = X_test_margin[~nan_indices]\n",
        "\n",
        "# Predict on the cleaned test set for Margin classification\n",
        "\n",
        "# Evaluate the model on the cleaned test set\n",
        "accuracy_margin_cleaned = accuracy_score(y_test_margin_cleaned, y_pred_margin)\n",
        "print(f'Accuracy for Margin classification after removing NaN values: {accuracy_margin_cleaned:.2f}')\n",
        "\n",
        "\n",
        "accuracy_size = accuracy_score(y_test_size, y_pred_size)\n",
        "print(f'Accuracy for Size classification: {accuracy_size:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5GUQx4eGc3H_",
      "metadata": {
        "id": "5GUQx4eGc3H_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J_k6JG8wc3Ek",
      "metadata": {
        "id": "J_k6JG8wc3Ek"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Et3f-xZIc3CE",
      "metadata": {
        "id": "Et3f-xZIc3CE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut,KFold\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8hawJZBXc2_c",
      "metadata": {
        "id": "8hawJZBXc2_c"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/College Purpose/SEM8/ads/dataset/creditcard_2023.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VArd-PNDdZbl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VArd-PNDdZbl",
        "outputId": "1c07a9c1-4a0f-4dae-b7a1-e9152f71e574"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Class\n",
              "0    284315\n",
              "1    284315\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134PsDrtddZq",
      "metadata": {
        "id": "134PsDrtddZq"
      },
      "outputs": [],
      "source": [
        "x = data.drop('Class', axis=1)  # Features\n",
        "y = data['Class']  # Target variable\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H61co4xndf2J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "H61co4xndf2J",
        "outputId": "78cbaff9-8417-4a75-b4e0-93df8e78d47c"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6dcc973d5751>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_tr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_tes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtes_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "kfold = KFold(n_splits = 5, shuffle = True, random_state =42)\n",
        "\n",
        "for tr_i ,tes_i in kfold.split(x):\n",
        "    x_tr , x_tes = x.iloc[tr_i],x.iloc[tes_i]\n",
        "    y_tr , y_tes = y.iloc[tr_i],y.iloc[tes_i]\n",
        "    model = RandomForestClassifier(random_state=42,n_estimators=100)\n",
        "    model.fit(x_tr,y_tr)\n",
        "    y_pred = model.predict(x_tes)\n",
        "    accuracy = accuracy_score(y_tes,y_pred)\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8sRZqgZHdiQc",
      "metadata": {
        "id": "8sRZqgZHdiQc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
